{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0ac5b7-a55b-4cc1-8c13-4bf92804ed37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fcn\n",
    "import pandas\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "from marel.services import ServiceProvider\n",
    "from marel.databricks import DatabricksProvider\n",
    "provider = DatabricksProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aff5c05-e8de-4ef9-b4fd-4c0a352562fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Months = 3\n",
    "df_sx_telemetry = spark.table('marel_digital_machines.sensorx.marel_sensorx_telemetry')\n",
    "df_sx_telemetry = df_sx_telemetry = df_sx_telemetry.filter(fcn.col('timeStamp') > fcn.add_months(fcn.current_date(),-Months)).select(\"timeStamp\",\"properties_deviceId\",\"payload_xrayController_filamentCurrent\",\"payload_xrayController_temperature\",\"payload_xrayController_tubeCurrent\", \"payload_xrayController_tubeVoltage\", \"payload_xrayController_onTime\")\n",
    "\n",
    "#display(df_sx_telemetry.limit(100))\n",
    "display(df_sx_telemetry.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f3c612-125a-4af5-8bc1-2236ad249d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Months = 3\n",
    "df_sx_serialnumber = (\n",
    "    spark.table(\"marel_digital_machines.sensorx.marel_sensorx_xraycontroller_property_serialnumber\")\n",
    "    .filter(fcn.col(\"timeStamp\") > fcn.add_months(fcn.current_date(), -Months))\n",
    "    .groupBy(\"properties_deviceId\", \"payload_serialNumber\")\n",
    "    .agg(\n",
    "        fcn.min(\"timeStamp\").alias(\"min_timeStamp\"),\n",
    "        fcn.max(\"timeStamp\").alias(\"max_timeStamp\")\n",
    "    )\n",
    ")\n",
    "display(df_sx_serialnumber.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c202f818-ebfc-452c-9eeb-7bd111d648c5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771418858343}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fcn\n",
    "\n",
    "BIN_SIZE_SECONDS = 86400\n",
    "\n",
    "T = df_sx_telemetry.alias(\"T\")\n",
    "S = df_sx_serialnumber.alias(\"S\")\n",
    "\n",
    "df_sx_telemetry_serialnumber = (\n",
    "    T.hint(\"range_join\", BIN_SIZE_SECONDS)\n",
    "     .join(\n",
    "        S,\n",
    "        (fcn.col(\"T.properties_deviceId\") == fcn.col(\"S.properties_deviceId\")) &\n",
    "        (fcn.col(\"T.timeStamp\") >= fcn.col(\"S.min_timeStamp\")) &\n",
    "        (fcn.col(\"T.timeStamp\") <= fcn.col(\"S.max_timeStamp\")),\n",
    "        how=\"left\"\n",
    "     )\n",
    "     # Keep ALL telemetry columns + just serial from S\n",
    "     .select(\"T.*\", fcn.col(\"S.payload_serialNumber\").alias(\"serialNumber\"))\n",
    ")\n",
    "\n",
    "df_one = df_sx_telemetry_serialnumber.filter(fcn.col(\"properties_deviceId\") == \"08947f89-b3c1-4b1e-ed33-08d9f153eeaf\")\n",
    "display(df_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ddae9b-9f4f-42a8-b358-722bf75184b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "Months = 1\n",
    "df_sx_xraycontroller_fault = spark.table('marel_digital_machines.sensorx.marel_sensorx_xraycontroller_property_fault')\n",
    "df_sx_xraycontroller_fault = df_sx_xraycontroller_fault.filter((fcn.col('timeStamp') > fcn.add_months(fcn.current_date(),-Months))).select(\"timeStamp\",\"properties_deviceId\",\"payload_fault_faultName\",\"payload_fault_state\")\n",
    "\n",
    "\n",
    "tolerance_seconds = 30\n",
    "\n",
    "m = df_sx_telemetry.alias(\"m\")\n",
    "l = df_sx_xraycontroller_fault.alias(\"l\")\n",
    "\n",
    "df_joined = (\n",
    "    m.join(\n",
    "        l,\n",
    "        (F.col(\"m.properties_deviceId\") == F.col(\"l.properties_deviceId\")) &\n",
    "        (F.abs(F.col(\"m.timeStamp\").cast(\"long\") - F.col(\"l.timeStamp\").cast(\"long\")) <= tolerance_seconds),\n",
    "        \"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_diff_seconds\",\n",
    "        F.abs(F.col(\"m.timeStamp\").cast(\"long\") - F.col(\"l.timeStamp\").cast(\"long\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# How many rows actually matched?\n",
    "matched = df_joined.filter(F.col(\"l.timeStamp\").isNotNull()).count()\n",
    "total = df_joined.count()\n",
    "print(\"matched rows:\", matched, \"out of\", total)\n",
    "\n",
    "display(df_joined)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cleaning_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
